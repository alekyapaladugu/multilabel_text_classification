# -*- coding: utf-8 -*-
"""CSC219Team5_FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xETFxxB8sghTphgXSCBDKvmbNIm0TFgR

# Multi Label Text Classification using Transformers

## 1.0 INSTALLATIONS AND UPLOADING THE DATASET
"""

!pip install scikit-multilearn
!pip install transformers

from google.colab import drive
drive.mount('/content/drive')

"""## 2.0 IMPORTS"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import os
import re
import sys
import copy
import time
import string
import warnings
from typing import Tuple, List
from functools import partial

import nltk
# nltk.download('reuters')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import reuters
from nltk.corpus import stopwords
from nltk import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import hamming_loss, f1_score
from sklearn.preprocessing import MultiLabelBinarizer
from skmultilearn.problem_transform import BinaryRelevance
from sklearn import metrics
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score, accuracy_score, classification_report
from skmultilearn.problem_transform import LabelPowerset
from sklearn.multioutput import ClassifierChain

import keras
from keras import initializers, regularizers, constraints, optimizers, layers
from keras.preprocessing.text import Tokenizer
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model

from tqdm import tqdm
import xgboost as xgb
from scipy.sparse import csr_matrix
from gensim.models import Word2Vec

import torch
from torch import optim
from torch import nn
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import Dataset, DataLoader, RandomSampler
from torch.nn.utils.rnn import pad_sequence
from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, BertPreTrainedModel

if not sys.warnoptions:
    warnings.simplefilter("ignore")

stop_words = set(stopwords.words('english'))
stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])
re_stop_words = re.compile(r"\b(" + "|".join(stop_words) + ")\\W", re.I)

stemmer = SnowballStemmer("english")

def plot_len_of_sentences(text_cols):
    comment_len = text_cols.len()
    # plot the distribution of comment lengths
    plt.figure(figsize=(8,4))
    sns.histplot(comment_len, kde=False, bins=50, color="red")
    plt.xlabel("Comment Length (Number of words)", fontsize=12)
    plt.ylabel("Number of Comments", fontsize=12)
    plt.title("Distribution of comment Lengths", fontsize=12)

    print("Average comment length:",int(sum(comment_len)/len(comment_len))," characters")

def plot_bar_graph(train_df,labels,title):
    fig = plt.figure(figsize=(8,6))
    ax = fig.add_axes([0,0,1,1])
    total_count = []
    for label in labels:
        total_count.append(len(train_df[train_df[label] == 1]))
    ax.bar(labels,total_count, color=['red', 'green', 'blue', 'purple', 'orange', 'yellow'])
    for i,data in enumerate(total_count):
        plt.text(i-.25,
                data/total_count[i]+100,
                total_count[i],
                fontsize=12)
    plt.title(title)
    plt.xlabel('Labels')
    plt.ylabel('Number of comments')

def cleanHtml(sentence):
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, ' ', str(sentence))
    return cleantext

def cleanPunc(sentence):
  #function to clean the word of any punctuation or special characters
    cleaned = re.sub(r'[?|!|\'|"|#]',r'',sentence)
    cleaned = re.sub(r'[.|,|)|(|\|/]',r' ',cleaned)
    cleaned = cleaned.strip()
    cleaned = cleaned.replace("\n"," ")
    return cleaned

def keepAlpha(sentence):
    alpha_sent = ""
    for word in sentence.split():
        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)
        alpha_sent += alpha_word
        alpha_sent += " "
    alpha_sent = alpha_sent.strip()
    return alpha_sent

def removeStopWords(sentence):
    global re_stop_words
    return re_stop_words.sub(" ", sentence)

def lemmatize(sentence):
    lemmatizer = WordNetLemmatizer()
    lemSentence = ""
    for word in sentence.split():
        lem = lemmatizer.lemmatize(word)
        lemSentence += lem
        lemSentence += " "
    lemSentence = lemSentence.strip()
    return lemSentence

def preprocess_text(train_df):
    train_df['comment_text'] = train_df['comment_text'].str.lower()
    train_df['comment_text'] = train_df['comment_text'].apply(cleanHtml)
    train_df['comment_text'] = train_df['comment_text'].apply(cleanPunc)
    train_df['comment_text'] = train_df['comment_text'].apply(keepAlpha)
    train_df['comment_text'] = train_df['comment_text'].apply(removeStopWords)
    train_df['comment_text'] = train_df['comment_text'].apply(lemmatize)
    return train_df

def sample_dataset(train_df):
    df = train_df[train_df['clean']==0]
    df = df.append(train_df[train_df['clean']==1].sample(frac=1, random_state=200).iloc[:df.shape[0],:])
    df = df.sample(frac=1,random_state=200)
    return df

#ML preprocessing
def baseline_ml_preprocessing(X_train,X_test):
    word_vectorizer = TfidfVectorizer(
        strip_accents='unicode',
        analyzer='word',
        max_features=3000,
        token_pattern=r'\w{1,}',
        ngram_range=(1, 3),
        stop_words='english',
        sublinear_tf=True)

    word_vectorizer.fit(X_train)
    ml_X_train = word_vectorizer.transform(X_train)
    ml_X_test = word_vectorizer.transform(X_test)
    return ml_X_train, ml_X_test

def Sequential_Preprocessing(X_train, X_test, y_train, y_test,max_seq_len):
    tokenizer = Tokenizer(num_words=20000,oov_token='<UNK>')
    tokenizer.fit_on_texts(X_train)
    X_train = tokenizer.texts_to_sequences(X_train)
    X_test = tokenizer.texts_to_sequences(X_test)

    X_train = torch.from_numpy(pad_sequences(X_train, maxlen=max_seq_len))
    X_test = torch.from_numpy(pad_sequences(X_test, maxlen=max_seq_len))

    y_train = torch.from_numpy(y_train)
    y_test = torch.from_numpy(y_test)

    return tokenizer, X_train, y_train, X_test, y_test

def create_glove_embedding_matrix(tokenizer, glove_file_path):
    glove_embeddings = {}
    with open(glove_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.split()
            word = parts[0]
            vector = np.array(parts[1:], dtype=np.float32)
            glove_embeddings[word] = vector

    vocab_size = len(tokenizer.index_word) + 1
    #File 300 dim expected
    embedding_dim = 300
    glove_embedding_matrix = torch.zeros(vocab_size, embedding_dim)

    unknown_words = 0
    for i in range(1, vocab_size):
        word = tokenizer.index_word[i]
        if word in glove_embeddings.keys():
            glove_embedding_matrix[i] = torch.from_numpy(glove_embeddings[word]).float()
        else:
            unknown_words += 1

    print("GloVe embedding matrix created!")
    print('Vocabulary size: {}'.format(vocab_size))
    print('Total unknown words: {}'.format(unknown_words))

    return glove_embeddings, glove_embedding_matrix

def create_glove_label_embedding(labels, glove_embeddings):
    glove_label_embedding = torch.zeros(len(labels), 300)

    for index, label in enumerate(labels):
        wrds = label.split('_')
        for l in wrds:
            if l in glove_embeddings.keys():
                glove_label_embedding[index] += torch.from_numpy(glove_embeddings[l])
        glove_label_embedding[index] /= len(wrds)

    return glove_label_embedding

def get_bert_embedding_matrix(tokenizer):
    bert_model = BertModel.from_pretrained('bert-base-uncased')
    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    vocab_size = len(tokenizer.index_word)+1
    bert_embedding_matrix = torch.zeros(vocab_size, 768)

    tokens_tensor=[]
    for i in tqdm(range(1, vocab_size)):
        word = tokenizer.index_word[i]
        tokens = bert_tokenizer.encode(word,add_special_tokens=False)[0]
        tokens_tensor.append(tokens)
    batch_size = 512
    vocab_batches = [tokens_tensor[i:i+batch_size] for i in range(0, len(tokens_tensor), batch_size)]

    with torch.no_grad():
      bert_embeddings = []
      for i in tqdm(range(len(vocab_batches))):
        batch = vocab_batches[i]
        inputs = {
        'input_ids': torch.tensor([batch]),
        'attention_mask': torch.ones(len(batch)).unsqueeze(0)
        }
        outputs = bert_model(**inputs)
        batch_embeddings = outputs[0][0]
        bert_embeddings.append(batch_embeddings)

    bert_embedding_matrix = torch.cat(bert_embeddings, dim=0)
    return bert_embedding_matrix

#Build adjacency matrix based on Co-Occurencies label
def create_adjacency_matrix_cooccurance(data_label):
  cooccur_matrix = np.zeros((data_label.shape[1], data_label.shape[1]), dtype=float)
  for y in data_label:
      y = list(y)
      for i in range(len(y)):
          for j in range(len(y)):
            #data_label
              if y[i] == 1 and y[j] == 1:
                  cooccur_matrix[i, j] += 1
  row_sums = data_label.sum(axis=0)

  for i in range(cooccur_matrix.shape[0]):
    for j in range(cooccur_matrix.shape[0]):
      if row_sums[i]!=0:
        cooccur_matrix[i][j]=cooccur_matrix[i,j]/row_sums[i]
      else:
        cooccur_matrix[i][j]=cooccur_matrix[i,j]

  return cooccur_matrix

def check_accuracy(model, label_embedding, X, y):
  model.eval()
  with torch.no_grad():
    out = model(X, label_embedding)
    y_pred = torch.sigmoid(out.detach()).round().cpu()
    f1score = f1_score(y, y_pred, average='micro')
    hammingloss = hamming_loss(y, y_pred)
  return hammingloss, f1score

class dataset(Dataset):
  def __init__(self, x, y):
    self.x  = x
    self.y = y
  def __len__(self):
    return len(self.x)

  def __getitem__(self, idx):
    return self.x[idx], self.y[idx]

def create_adjacency_matrix_xavier(data_label):
  adj_matrix = torch.empty((data_label.shape[1], data_label.shape[1]))
  adj_matrix = nn.init.xavier_uniform_
  (adj_matrix)
  return adj_matrix

"""## 3.0 DATA PREPROCESSING FOR MACHINE LEARNING"""

train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CSC219-MachineLearning/finalproject/jigsaw-toxic-comment-classification-challenge/train.csv')
print("Data shape:",train_df.shape)
train_df.info()

#Adding new column of Clean Text
arr=[]
for i in range(train_df.shape[0]):
  if (train_df.iloc[i,2:]==0).all():
    arr.append(1)
  else:
    arr.append(0)
train_df['clean'] = pd.Series(np.asarray(arr))
labels = train_df.columns[2:]
print("Labels:",labels)

train_df.head()

plot_len_of_sentences(train_df.comment_text.str)

#Without clean Text
plot_bar_graph(train_df,labels[:-1],'Number of comments per label- Without CLEAN column')

#With clean Text
plot_bar_graph(train_df,labels,'Number of comments per label- With CLEAN column')

train_df = preprocess_text(train_df)
train_df.head()

#Get sampled Data by removing Rows which create Bias
train_df = sample_dataset(train_df)
train_df.head()

#modified dataset
plot_bar_graph(train_df,labels,'Processed Balanced Data with columns')

X = train_df['comment_text'].values
y = train_df.iloc[:,2:].values
print(X.shape, y.shape)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=200, stratify=y[:,-1])
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape )

ml_X_train, ml_X_test = baseline_ml_preprocessing(X_train,X_test)
print("X_train shape for baseline models:",ml_X_train.shape)
print("X_test shape for baseline models:",ml_X_test.shape)

"""## 4.0 CLASSICAL ML ALGORITHMS

We test the following algorithms -

*   Binary Relevance
*   Classifier Chain
*   One Vs Rest
*   Label PowerSet
*   Hierarchical SVM
"""

class HierarchicalSVM:
    def __init__(self):
        self.classifiers = {}#Dictionary to store the classifiers for each label

    def train(self, X_train, Y_train):
        num_labels = Y_train.shape[1]
        # Train the classifiers for each label
        for label_idx in range(num_labels):
            label = str(label_idx)
            Y_label = Y_train[:, label_idx]
            classifier = SVC(kernel='linear')
            classifier.fit(X_train, Y_label)
            self.classifiers[label] = classifier

    def predict(self, X_test):
        num_labels = len(self.classifiers)
        num_samples = X_test.shape[0]
        predictions = np.zeros((num_samples, num_labels))
        for label_idx, classifier in self.classifiers.items():
            label_predictions = classifier.predict(X_test)
            predictions[:, int(label_idx)] = label_predictions
        return predictions

# Initialize an empty DataFrame for scores
scores_df = pd.DataFrame()

# Helper function to add results to the DataFrame
def add_to_scores_df(arr):
    global scores_df
    scores_df = pd.concat([scores_df, pd.DataFrame([arr])], ignore_index=True)

# Binary Relevance
print("====================BR============================")
model = BinaryRelevance(LogisticRegression(solver='sag'))
model.fit(ml_X_train, y_train[:,:-1])
y_pred = model.predict(ml_X_test)
arr = {'model_name':'BR',
       'micro_avg_f1_score':f1_score(y_test[:,:-1], y_pred, average="micro"),
       'hamming_loss':hamming_loss(y_test[:,:-1], y_pred),
       'accuracy':accuracy_score(y_test[:,:-1], y_pred)}
add_to_scores_df(arr)
print(classification_report(y_test[:,:-1], y_pred, zero_division=0))

# Classifier Chain
print("====================Classifier Chain============================")
model = ClassifierChain(LogisticRegression(solver='sag'))
model.fit(ml_X_train, y_train[:,:-1])
y_pred = model.predict(ml_X_test)
arr = {'model_name':'Classifier_Chain',
       'micro_avg_f1_score':f1_score(y_test[:,:-1], y_pred, average="micro"),
       'hamming_loss':hamming_loss(y_test[:,:-1], y_pred),
       'accuracy':accuracy_score(y_test[:,:-1], y_pred)}
add_to_scores_df(arr)
print(classification_report(y_test[:,:-1], y_pred, zero_division=0))

# One Vs Rest
print("====================One Vs Rest============================")
model = OneVsRestClassifier(LogisticRegression())
model.fit(ml_X_train, y_train[:,:-1])
y_pred = model.predict(ml_X_test)
arr = {'model_name':'One_vs_rest',
       'micro_avg_f1_score':f1_score(y_test[:,:-1], y_pred, average="micro"),
       'hamming_loss':hamming_loss(y_test[:,:-1], y_pred),
       'accuracy':accuracy_score(y_test[:,:-1], y_pred)}
add_to_scores_df(arr)
print(classification_report(y_test[:,:-1], y_pred, zero_division=0))

# Label Powerset
print("====================Label PowerSet============================")
model = LabelPowerset(LogisticRegression(max_iter=1000))
model.fit(ml_X_train, y_train[:,:-1])
y_pred = model.predict(ml_X_test)
arr = {'model_name':'Label_Powerset',
       'micro_avg_f1_score':f1_score(y_test[:,:-1], y_pred, average="micro"),
       'hamming_loss':hamming_loss(y_test[:,:-1], y_pred),
       'accuracy':accuracy_score(y_test[:,:-1], y_pred)}
add_to_scores_df(arr)
print(classification_report(y_test[:,:-1], y_pred, zero_division=0))

# Hierarchical SVM
print("====================Hierarchical SVM============================")
model = HierarchicalSVM()
model.train(ml_X_train, y_train[:,:-1])
y_pred = model.predict(ml_X_test)
arr = {'model_name':'Hierarchical_SVM',
       'micro_avg_f1_score':f1_score(y_test[:,:-1], y_pred, average="micro"),
       'hamming_loss':hamming_loss(y_test[:,:-1], y_pred),
       'accuracy':accuracy_score(y_test[:,:-1], y_pred)}
add_to_scores_df(arr)
print(classification_report(y_test[:,:-1], y_pred, zero_division=0))

scores_df

"""## 5.0 DATA PREPROCESSING FOR DEEP LEARNING"""

max_seq_len = 128
tokenizer, X_train, y_train, X_test, y_test = Sequential_Preprocessing(X_train, X_test, y_train, y_test,max_seq_len)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

glove_embeddings, glove_embedding_matrix = create_glove_embedding_matrix(tokenizer,glove_file_path='/content/drive/MyDrive/Colab Notebooks/CSC219-MachineLearning/finalproject/glove.6B/glove.6B.300d.txt')

glove_label_embeddings = create_glove_label_embedding(labels,glove_embeddings)
print(glove_label_embeddings.shape)

# word2vec_model = Word2Vec.load('path/to/word2vec_model')

# word2vec_embedding = {word: word2vec_model.wv[word] for word in word2vec_model.wv.vocab}

# # Define the vocabulary size and embedding dimensions
# vocab_size = len(tokenizer.index_word)+1
# embedding_dim = word2vec_model.vector_size

# # Create the embedding matrix
# word2vec_embedding_matrix = np.zeros((vocab_size, embedding_dim))
# for i, word in enumerate(word2vec_embedding):
#     word = tokenizer.index_word[i]
#     if word in word2vec_embedding.keys():
#       word2vec_embedding_matrix[i] = word2vec_embedding[word]
#     else:
#       continue
# print("Word2Vec Embedding matrix created!")
# print(word2vec_embedding_matrix.shape)

# word2vec_label_embedding = torch.zeros(len(labels),embedding_dim)

# for index, label in enumerate(labels):
#   wrds = label.split('_')
#   for l in wrds:
#     if l in word2vec_embedding.keys():
#         word2vec_label_embedding[index] +=  torch.from_numpy(word2vec_embedding[l])
#   word2vec_label_embedding[index]=word2vec_label_embedding[index]/len(wrds)

# print("Word2Vec label embedding:")
# print(word2vec_label_embedding)

bert_embeddding_matrix = get_bert_embedding_matrix(tokenizer)
print(bert_embeddding_matrix.shape)

#Checking GPU for tensorflow
# sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))
# keras.backend.set_session(sess)
# print("GPU is Available? =>",tf.test.is_gpu_available())

VOCAB_SIZE = len(tokenizer.index_word)+1

#Preparing for KERAS Model
glove_embd_mat= np.array(glove_embedding_matrix.to('cpu'))
glove_embd_mat = np.vstack((np.zeros((1, glove_embd_mat.shape[1])), glove_embd_mat))
bert_embd_mat= np.array(bert_embeddding_matrix.to('cpu').detach().numpy())
bert_embd_mat = np.vstack((np.zeros((1, bert_embd_mat.shape[1])), bert_embd_mat))

"""## 6.0 DEEP LEARNING ALGORITHMS

* BiLSTM using glove embeddings
* BiLSTM using BERT embeddings
* CNN using BERT embeddings
* CNN usin BERT embeddings

### 6.1 BiLSTM WITH GLOVE EMBEDDINGS
"""

# GloVe BiLSTM Model
print("=====================Glove BiLSTM===========================")

inp = Input(shape=(max_seq_len,))
x = Embedding(VOCAB_SIZE+1, 300, weights=[glove_embd_mat])(inp)
x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)
x = GlobalMaxPool1D()(x)
x = Dense(50, activation="relu")(x)
x = Dropout(0.1)(x)
x = Dense(7, activation="sigmoid")(x)
model = Model(inputs=inp, outputs=x)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(np.asarray(X_train), np.asarray(y_train), batch_size=256, epochs=3)
y_pred = model.predict(np.asarray(X_test), batch_size=256, verbose=1)
y_pred = np.where(y_pred > 0.5, 1, 0)

arr = {'model_name':'BiLSTM_glove',
       'micro_avg_f1_score':f1_score(y_test, y_pred, average="micro"),
       'hamming_loss':hamming_loss(y_test, y_pred),
       'accuracy':accuracy_score(y_test, y_pred)}
add_to_scores_df(arr)
print(scores_df)
print(classification_report(y_test, y_pred, zero_division=0))

"""### 6.2 BiLSTM WITH BERT EMBEDDINGS"""

# Bert BiLSTM Model
print("=====================Bert BiLSTM===========================")

inp = Input(shape=(max_seq_len,))
x = Embedding(VOCAB_SIZE, 768, weights=[bert_embd_mat])(inp)
x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)
x = GlobalMaxPool1D()(x)
x = Dense(50, activation="relu")(x)
x = Dropout(0.1)(x)
x = Dense(7, activation="sigmoid")(x)
model = Model(inputs=inp, outputs=x)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(np.asarray(X_train), np.asarray(y_train), batch_size=256, epochs=4)
y_pred = model.predict(np.asarray(X_test), batch_size=256, verbose=1)
y_pred = np.where(y_pred > 0.5, 1, 0)

arr = {'model_name':'BiLSTM_bert',
       'micro_avg_f1_score':f1_score(y_test, y_pred, average="micro"),
       'hamming_loss':hamming_loss(y_test, y_pred),
       'accuracy':accuracy_score(y_test, y_pred)}
add_to_scores_df(arr)
print(scores_df)
print(classification_report(y_test, y_pred, zero_division=0))

"""### 6.3 CNN WITH GLOVE EMBEDDINGS"""

# Glove CNN Model
print("=====================Glove CNN===========================")

inp = Input(shape=(max_seq_len,))
x = Embedding(VOCAB_SIZE+1, 300, weights=[glove_embd_mat])(inp)
x = layers.Conv1D(filters=128, kernel_size=5, activation='relu')(x)
x = GlobalMaxPool1D()(x)
x = Dense(64, activation="relu")(x)
x = Dropout(0.1)(x)
x = Dense(32, activation='relu')(x)
x = Dense(7, activation="sigmoid")(x)
model = Model(inputs=inp, outputs=x)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(np.asarray(X_train), np.asarray(y_train), batch_size=256, epochs=3)
y_pred = model.predict(np.asarray(X_test), batch_size=256, verbose=1)
y_pred = np.where(y_pred > 0.5, 1, 0)

arr = {'model_name':'CNN_glove',
       'micro_avg_f1_score':f1_score(y_test, y_pred, average="micro"),
       'hamming_loss':hamming_loss(y_test, y_pred),
       'accuracy':accuracy_score(y_test, y_pred)}
add_to_scores_df(arr)
print(scores_df)
print(classification_report(y_test, y_pred, zero_division=0))

"""### 6.4 CNN WITH BERT EMBEDDINGS"""

# Bert CNN Model
print("=====================Bert CNN===========================")

inp = Input(shape=(max_seq_len,))
x = Embedding(VOCAB_SIZE, 768, weights=[bert_embd_mat])(inp)
x = layers.Conv1D(filters=128, kernel_size=5, activation='relu')(x)
x = GlobalMaxPool1D()(x)
x = Dense(64, activation="relu")(x)
x = Dropout(0.1)(x)
x = Dense(32, activation='relu')(x)
x = Dense(7, activation="sigmoid")(x)
model = Model(inputs=inp, outputs=x)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(np.asarray(X_train), np.asarray(y_train), batch_size=256, epochs=3)
y_pred = model.predict(np.asarray(X_test), batch_size=256, verbose=1)
y_pred = np.where(y_pred > 0.5, 1, 0)

arr = {'model_name':'CNN_bert',
       'micro_avg_f1_score':f1_score(y_test, y_pred, average="micro"),
       'hamming_loss':hamming_loss(y_test, y_pred),
       'accuracy':accuracy_score(y_test, y_pred)}
add_to_scores_df(arr)
print(scores_df)
print(classification_report(y_test, y_pred, zero_division=0))

scores_df

"""### 7.0 MAGNET MODEL"""

def train(model, X_train, label_embedding, y_train,
          total_epoch=25, batch_size=32, learning_rate=0.001,
          save_path='./model.pt', state=None):

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    label_embedding = label_embedding.to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    train_data = DataLoader(dataset(X_train, y_train), batch_size=batch_size)

    best_loss = float('inf')
    num_increasing_epochs = 0

    for epoch in range(1, total_epoch + 1):
        running_loss = 0
        y_pred = []
        epoch_time = 0
        model.train()

        for index, (X, y) in enumerate(train_data):
            optimizer.zero_grad()
            out = model(X.to(device), label_embedding)
            loss = criterion(out, y.to(device).float())
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)
            optimizer.step()
            y_pred.append(torch.sigmoid(out.detach()).round().cpu())
            running_loss += loss.item()

        y_pred = torch.vstack(y_pred)
        f1score = f1_score(y_train, y_pred, average='micro')
        hammingloss = hamming_loss(y_train, y_pred)
        print(f'epoch:{epoch} loss:{running_loss:.5f} hamming_loss:{hammingloss:.5f} micro_f1score:{f1score:.5f}')

class MAGNET(nn.Module):
  def __init__(self, input_size, hidden_size, adjacency, embeddings, heads=4, slope=0.01, dropout=0.5):
    super(MAGNET, self).__init__()
    self.embedding = nn.Embedding.from_pretrained(embeddings)
    self.biLSTM = nn.LSTM(input_size,hidden_size,batch_first=True,bidirectional=True)
    self.adjacency = nn.Parameter(adjacency)
    self.dropout = nn.Dropout(dropout)
    self.edge_weights = nn.Linear(hidden_size*2*2, 1, bias=False)
    self.activation = nn.LeakyReLU(slope)
    self.softmax = nn.Softmax(dim=1)
    self.tanh = nn.Tanh()
    self.heads = heads
    self.transform_dim1 = nn.Linear(input_size, hidden_size*2, bias=False)
    self.transform_dim2 = nn.Linear(hidden_size*2, hidden_size*2, bias=False)
    self.transform_dimensions = [self.transform_dim1, self.transform_dim2]

  def forward(self, token, label_embedding):
      #BILSTM part
      features = self.embedding(token)
      out, (h, _) = self.biLSTM(features)
      embedding = torch.cat([h[-2, :, :], h[-1, :, :]], dim=1)
      embedding = self.dropout(embedding)

      #GAT PART
      for td in self.transform_dimensions: #Two Multiheaded GAT layers
        outputs = []
        for head in range(self.heads):
          label_embed = td(label_embedding)
          n, embed_size = label_embed.shape

          label_embed_combinations = label_embed.unsqueeze(1).expand(-1, n, -1)
          label_embed_combinations = torch.cat([label_embed_combinations, label_embed.unsqueeze(0).expand(n, -1, -1)], dim=2)
          e = self.activation(self.edge_weights(label_embed_combinations).squeeze(2))

          attention_coefficients = self.tanh(torch.mul(e,self.adjacency))

          new_h = torch.matmul(attention_coefficients.to(label_embed.dtype), label_embed)
          outputs.append(new_h)
        outputs = self.activation(torch.mean(torch.stack(outputs, dim=0),dim=0))

        label_embedding = outputs
      attention_features = self.dropout(label_embedding)
      attention_features = attention_features.transpose(0, 1)
      predicted_labels = torch.matmul(embedding, attention_features)
      return predicted_labels

save_path = '/content/drive/MyDrive/Colab Notebooks/CSC219-MachineLearning/finalproject/glove_magnet_model.pth'

"""### 7.1 INITIALIZAATION USING COOCCURANCE MATRIX"""

adj_matrix = create_adjacency_matrix_cooccurance(y_train.numpy())
adj_matrix = torch.tensor(adj_matrix)
adj_matrix

model = MAGNET(300, 250, adj_matrix, glove_embedding_matrix,heads=8)

#Coocurance initialization
train(model, X_train, glove_label_embeddings,y_train,total_epoch=5, save_path=save_path)

torch.save(model,save_path)

y_pred = []
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
for i in tqdm(range(X_test.shape[0])):
  x = X_test[i].view(1,128)
  out = model(x.to(device), glove_label_embeddings.to(device))
  y_pred.append(torch.sigmoid(out.detach()).round().cpu())

y_pred = torch.vstack(y_pred)

# Add MAGNET_Cooccurance model's metrics to scores_df
arr = {'model_name':'MAGNET_Cooccurance',
       'micro_avg_f1_score':f1_score(y_test, y_pred, average="micro"),
       'hamming_loss':hamming_loss(y_test, y_pred),
       'accuracy':accuracy_score(y_test, y_pred)}
add_to_scores_df(arr)

print(arr)
print(classification_report(y_test, y_pred, zero_division=0))

"""### 7.2 XAVIOUR'S INITIALIZATION"""

adj_matrix = create_adjacency_matrix_xavier(y_train.numpy())
adj_matrix = torch.tensor(adj_matrix)
adj_matrix

model = MAGNET(300, 250, adj_matrix, glove_embedding_matrix,heads=8)

#Xavier Initialization
train(model, X_train, glove_label_embeddings,y_train,total_epoch=5, save_path=save_path)

y_pred = []
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
for i in tqdm(range(X_test.shape[0])):
  x = X_test[i].view(1,128)
  out = model(x.to(device), glove_label_embeddings.to(device))
  y_pred.append(torch.sigmoid(out.detach()).round().cpu())

y_pred = torch.vstack(y_pred)

# Add MAGNET_xavier model's metrics to scores_df
arr = {'model_name':'MAGNET_xavier',
       'micro_avg_f1_score':f1_score(y_test, y_pred, average="micro"),
       'hamming_loss':hamming_loss(y_test, y_pred),
       'accuracy':accuracy_score(y_test, y_pred)}
add_to_scores_df(arr)

print(arr)
print(classification_report(y_test, y_pred, zero_division=0))

"""### 7.3 RANDOM INITIALIZATION"""

#Random initialisation
adj_matrix = torch.randn(7, 7)

model = MAGNET(300, 250, adj_matrix, glove_embedding_matrix,heads=8)

#Random initialisation
train(model, X_train, glove_label_embeddings,y_train,total_epoch=5, save_path=save_path)

y_pred = []
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
for i in tqdm(range(X_test.shape[0])):
  x = X_test[i].view(1,128)
  out = model(x.to(device), glove_label_embeddings.to(device))
  y_pred.append(torch.sigmoid(out.detach()).round().cpu())

y_pred = torch.vstack(y_pred)

# Add MAGNET_random model's metrics to scores_df
arr = {'model_name':'MAGNET_random',
       'micro_avg_f1_score':f1_score(y_test, y_pred, average="micro"),
       'hamming_loss':hamming_loss(y_test, y_pred),
       'accuracy':accuracy_score(y_test, y_pred)}
add_to_scores_df(arr)

print(arr)
print(classification_report(y_test, y_pred, zero_division=0))

model.adjacency

new_h = model.adjacency @ glove_label_embeddings.to(device).float()

"""### 7.4 OBSERVATION"""

from sklearn.decomposition import PCA
pca = PCA(2)
Xt = pca.fit_transform(new_h.cpu().detach().numpy())
plot = plt.scatter(Xt[:,0], Xt[:,1])
for i, label in enumerate(labels):
    plt.annotate(label, (Xt[i][0], Xt[i][1]), textcoords="offset points", xytext=(0, 5), ha='center')

plt.xlim(-6, 20)
plt.ylim(-10, 10)
plt.show()

from sklearn.decomposition import PCA
pca = PCA(2)
Xt = pca.fit_transform(glove_label_embeddings)
plot = plt.scatter(Xt[:,0], Xt[:,1])
for i, label in enumerate(labels):
    plt.annotate(label, (Xt[i][0], Xt[i][1]), textcoords="offset points", xytext=(0, 5), ha='center')

plt.xlim(-6, 20)
plt.ylim(-10, 10)
plt.show()

plt.imshow(adj_matrix, cmap='hot', interpolation='nearest')
plt.colorbar()
plt.xticks(np.arange(len(labels)), labels, rotation=45)
plt.title('Initialized Adjacency Matrix HeatMap')
plt.show()

plt.imshow(model.adjacency.detach().cpu().numpy(), cmap='hot', interpolation='nearest')
plt.colorbar()
plt.xticks(np.arange(len(labels)), labels, rotation=45)
plt.title('Learned Adjacency Matrix HeatMap')
plt.show()

scores_df

"""## 8.0 BERT MODEL"""

#train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CSC219-MachineLearning/finalproject/jigsaw-toxic-comment-classification-challenge/train.csv')

train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CSC219-MachineLearning/finalproject/train_bert.csv')

print("Data shape:",train_df.shape)
#assigning labels

#Adding new column of Clean Text
arr=[]
for i in range(train_df.shape[0]):
  if (train_df.iloc[i,2:]==0).all():
    arr.append(1)
  else:
    arr.append(0)
train_df['clean'] = pd.Series(np.asarray(arr))
labels = train_df.columns[2:]
print("Labels:",labels)

train_df = preprocess_text(train_df)
train_df.head()

#Get sampled Data by removing Rows which create Bias
train_df = sample_dataset(train_df)
train_df.head()

device = torch.device('cuda')
if torch.cuda.is_available():
    device = torch.device('cuda:0')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

train_df, test_df = train_test_split(train_df, test_size=0.2,random_state=200,stratify=train_df.iloc[:,-1])

class ToxicDataset(Dataset):

    def __init__(self, tokenizer: BertTokenizer, dataframe: pd.DataFrame, lazy: bool = False):
        self.tokenizer = tokenizer
        self.pad_idx = tokenizer.pad_token_id
        self.lazy = lazy
        if not self.lazy:
            self.X = []
            self.Y = []
            for i, (row) in tqdm(dataframe.iterrows()):
                x, y = self.row_to_tensor(self.tokenizer, row)
                self.X.append(x)
                self.Y.append(y)
        else:
            self.df = dataframe

    @staticmethod
    def row_to_tensor(tokenizer: BertTokenizer, row: pd.Series) -> Tuple[torch.LongTensor, torch.LongTensor]:
        tokens = tokenizer.encode(row["comment_text"], add_special_tokens=True)
        if len(tokens) > 120:
            tokens = tokens[:119] + [tokens[-1]]
        x = torch.LongTensor(tokens)
        y = torch.FloatTensor(row[["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate","clean"]])
        return x, y


    def __len__(self):
        if self.lazy:
            return len(self.df)
        else:
            return len(self.X)

    def __getitem__(self, index: int) -> Tuple[torch.LongTensor, torch.LongTensor]:
        if not self.lazy:
            return self.X[index], self.Y[index]
        else:
            return self.row_to_tensor(self.tokenizer, self.df.iloc[index])


def collate_fn(batch: List[Tuple[torch.LongTensor, torch.LongTensor]], device: torch.device) \
        -> Tuple[torch.LongTensor, torch.LongTensor]:
    x, y = list(zip(*batch))
    x = pad_sequence(x, batch_first=True, padding_value=0)
    y = torch.stack(y)
    return x.to(device), y.to(device)

train_dataset = ToxicDataset(tokenizer, train_df, lazy=True)
test_dataset = ToxicDataset(tokenizer, test_df, lazy=True)
collate_fn = partial(collate_fn, device=device)
BATCH_SIZE = 32
train_sampler = RandomSampler(train_dataset)
dev_sampler = RandomSampler(test_dataset)
train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)
dev_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=dev_sampler, collate_fn=collate_fn)

class BertClassifier(nn.Module):

    def __init__(self, bert: BertModel, num_classes: int):
        super().__init__()
        self.bert = bert
        self.classifier = nn.Linear(bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,

            labels=None):
        outputs = self.bert(input_ids,
                               attention_mask=attention_mask,
                               token_type_ids=token_type_ids,
                               position_ids=position_ids,
                               head_mask=head_mask)
        cls_output = outputs[1] # batch, hidden
        cls_output = self.classifier(cls_output) # batch, 6
        cls_output = torch.sigmoid(cls_output)
        criterion = nn.BCELoss()
        loss = 0
        if labels is not None:
            loss = criterion(cls_output, labels)
        return loss, cls_output

def train(model, iterator, optimizer, scheduler):
    model.train()
    total_loss = 0
    for x, y in tqdm(iterator):
        optimizer.zero_grad()
        mask = (x != 0).float()
        loss, outputs = model(x, attention_mask=mask, labels=y)
        total_loss += loss.item()
        loss.backward()
        optimizer.step()
        scheduler.step()
    print(f"Train loss {total_loss / len(iterator)}")

! nvidia-smi

! nvidia-smi -c 0

model = BertClassifier(BertModel.from_pretrained('bert-base-uncased'), 7).to(device)

no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]
EPOCH_NUM = 8
# triangular learning rate, linearly grows untill half of first epoch, then linearly decays
warmup_steps = 10 ** 3
total_steps = len(train_iterator) * EPOCH_NUM - warmup_steps
optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)
scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)
# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=total_steps)

for i in range(EPOCH_NUM):
    print('=' * 50, f"EPOCH {i}", '=' * 50)
    train(model, train_iterator, optimizer, scheduler)
    # evaluate(model, dev_iterator)

model.eval()
y_pred=[]
columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate','clean']
for i in tqdm(range(len(test_df) // BATCH_SIZE + 1)):
    batch_df = test_df.iloc[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]
    texts = []
    for text in batch_df["comment_text"].tolist():
        text = tokenizer.encode(text, add_special_tokens=True)
        if len(text) > 120:
            text = text[:119] + [tokenizer.sep_token_id]
        texts.append(torch.LongTensor(text))
    x = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)
    mask = (x != tokenizer.pad_token_id).float().to(device)
    with torch.no_grad():
        _, outputs = model(x, attention_mask=mask)
    outputs = outputs.cpu().numpy()
    y_pred.append(outputs)

y_pred = np.vstack(np.asarray(y_pred))

y_pred = np.where(y_pred>0.5,1,0)

y_test = test_df.iloc[:,2:].values

# Add BERT model's metrics to scores_df
arr = {'model_name':'BERT',
       'micro_avg_f1_score':f1_score(y_test, y_pred, average="micro"),
       'hamming_loss':hamming_loss(y_test, y_pred),
       'accuracy':accuracy_score(y_test, y_pred)}
add_to_scores_df(arr)

print(arr)
print(classification_report(y_test, y_pred, zero_division=0))

scores_df